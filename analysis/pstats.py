# -*- coding: utf-8 -*-
"""
ë°ì´í„° íƒìƒ‰ ë° ì‹œê°í™” ì „ë¬¸ê°€ìš© GUI ì• í”Œë¦¬ì¼€ì´ì…˜
Description:
    ì´ ì• í”Œë¦¬ì¼€ì´ì…˜ì€ ë‘ ê°œì˜ jsonl ë°ì´í„°ì…‹ì„ ì…ë ¥ë°›ì•„, í…ìŠ¤íŠ¸ í†µê³„,
    ì–´íœ˜ ë‹¤ì–‘ì„±, ì„ë² ë”© ê¸°ë°˜ ì˜ë¯¸ ê³µê°„ ë¶„ì„ì„ ìˆ˜í–‰í•˜ê³  ê·¸ ê²°ê³¼ë¥¼ ì‹œê°í™”í•©ë‹ˆë‹¤.
    ëª¨ë“  ë¶„ì„ ê³¼ì •ì€ ì²´ê³„ì ìœ¼ë¡œ ìë™í™”ë˜ì–´ ìˆìœ¼ë©°, ê²°ê³¼ëŠ” íƒ€ì„ìŠ¤íƒ¬í”„ í´ë”ì— ì €ì¥ë©ë‹ˆë‹¤.
Author:
    ë°ì´í„°ê³¼í•™ ë° íŒŒì´ì¬ ì „ë¬¸ê°€ (100k USD/day)
Date:
    2024-06-17
"""
import os
import re
import json
import logging
import threading
import queue
from pathlib import Path
from datetime import datetime
import tkinter as tk
from tkinter import ttk, filedialog, scrolledtext

# --- ë°ì´í„° ë¶„ì„ ë° ì‹œê°í™” ë¼ì´ë¸ŒëŸ¬ë¦¬ ---
import pandas as pd
import numpy as np
import matplotlib
matplotlib.use('Agg') # GUI ë°±ì—”ë“œ ì¶©ëŒ ë°©ì§€
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.manifold import TSNE
from sklearn.metrics.pairwise import cosine_similarity
from konlpy.tag import Okt
from wordcloud import WordCloud

# í•œêµ­ì–´ í°íŠ¸ ì„¤ì • (ë§‘ì€ ê³ ë”•)
plt.rcParams['font.family'] = 'Malgun Gothic'
plt.rcParams['axes.unicode_minus'] = False


# --- 1. ë¶„ì„ ë¡œì§ í•¨ìˆ˜ (Core Analysis Functions) ---

def analyze_text_length(df: pd.DataFrame, dataset_name: str) -> pd.DataFrame:
    """ë°ì´í„°í”„ë ˆì„ì˜ 'question' í•„ë“œì— ëŒ€í•œ í…ìŠ¤íŠ¸ ê¸¸ì´ í†µê³„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
    logging.info(f"[{dataset_name}] í…ìŠ¤íŠ¸ ê¸¸ì´ ë¶„ì„ ì‹œì‘...")
    # ê¸€ì ìˆ˜
    df['char_count'] = df['question'].str.len()
    # ë‹¨ì–´ ìˆ˜ (ê³µë°± ê¸°ì¤€)
    df['word_count'] = df['question'].str.split().str.len()
    # ë¬¸ì¥ ìˆ˜ (ë§ˆì¹¨í‘œ, ë¬¼ìŒí‘œ, ëŠë‚Œí‘œ ê¸°ì¤€)
    df['sentence_count'] = df['question'].str.count(r'[.?!]') + 1
    
    stats = df[['char_count', 'word_count', 'sentence_count']].describe().round(2)
    logging.info(f"[{dataset_name}] í…ìŠ¤íŠ¸ ê¸¸ì´ ê¸°ìˆ  í†µê³„:\n{stats}")
    return df

def visualize_length_distributions(df1: pd.DataFrame, df2: pd.DataFrame, output_dir: Path):
    """ë‘ ë°ì´í„°ì…‹ì˜ í…ìŠ¤íŠ¸ ê¸¸ì´ ë¶„í¬ë¥¼ ì‹œê°í™”í•©ë‹ˆë‹¤."""
    logging.info("í…ìŠ¤íŠ¸ ê¸¸ì´ ë¶„í¬ ì‹œê°í™” ìƒì„± ì¤‘...")
    fig, axes = plt.subplots(1, 3, figsize=(18, 5))
    metrics = ['char_count', 'word_count', 'sentence_count']
    titles = ['ê¸€ì ìˆ˜ ë¶„í¬', 'ë‹¨ì–´ ìˆ˜ ë¶„í¬', 'ë¬¸ì¥ ìˆ˜ ë¶„í¬']

    for ax, metric, title in zip(axes, metrics, titles):
        sns.histplot(df1[metric], ax=ax, color='skyblue', label='Dataset 1', kde=True, stat="density")
        sns.histplot(df2[metric], ax=ax, color='salmon', label='Dataset 2', kde=True, stat="density")
        ax.set_title(title)
        ax.legend()
    
    plt.tight_layout()
    save_path = output_dir / "1_text_length_distribution.png"
    plt.savefig(save_path)
    plt.close()
    logging.info(f"ì‹œê°í™” ì €ì¥ ì™„ë£Œ: {save_path}")

def analyze_lexical_diversity(df: pd.DataFrame, okt: Okt, dataset_name: str) -> dict:
    """ë°ì´í„°ì…‹ì˜ ì–´íœ˜ ë‹¤ì–‘ì„±(TTR) ë° ìƒìœ„ í‚¤ì›Œë“œ(TF-IDF)ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤."""
    logging.info(f"[{dataset_name}] ì–´íœ˜ ë‹¤ì–‘ì„± ë° í‚¤ì›Œë“œ ë¶„ì„ ì‹œì‘...")
    
    # í† í°í™” (ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŒ)
    tokens = [okt.morphs(text) for text in df['question']]
    
    # TTR ë° ê³ ìœ  ì–´íœ˜ ìˆ˜ ê³„ì‚°
    all_tokens = [token for sublist in tokens for token in sublist]
    total_tokens = len(all_tokens)
    unique_tokens = len(set(all_tokens))
    ttr = unique_tokens / total_tokens if total_tokens > 0 else 0
    
    results = {
        'total_tokens': total_tokens,
        'unique_tokens': unique_tokens,
        'ttr': ttr
    }
    logging.info(f"[{dataset_name}] ì–´íœ˜ ë¶„ì„ ê²°ê³¼: {results}")
    
    # TF-IDF ê³„ì‚°
    corpus = [' '.join(token_list) for token_list in tokens]
    vectorizer = TfidfVectorizer(max_features=2000, min_df=5, max_df=0.5)
    tfidf_matrix = vectorizer.fit_transform(corpus)
    
    feature_names = vectorizer.get_feature_names_out()
    scores = tfidf_matrix.sum(axis=0).A1
    tfidf_scores = dict(zip(feature_names, scores))
    
    # ìƒìœ„ 30ê°œ í‚¤ì›Œë“œ
    top_keywords = sorted(tfidf_scores.items(), key=lambda x: x[1], reverse=True)[:30]
    results['top_keywords'] = top_keywords
    results['tfidf_scores_for_wc'] = tfidf_scores

    logging.info(f"[{dataset_name}] ìƒìœ„ í‚¤ì›Œë“œ (TF-IDF):\n{top_keywords[:10]}")
    return results

def visualize_lexical_analysis(res1: dict, res2: dict, output_dir: Path):
    """ì–´íœ˜ ë¶„ì„ ê²°ê³¼ë¥¼ ë§‰ëŒ€ê·¸ë˜í”„ì™€ ì›Œë“œí´ë¼ìš°ë“œë¡œ ì‹œê°í™”í•©ë‹ˆë‹¤."""
    logging.info("ì–´íœ˜ ë¶„ì„ ê²°ê³¼ ì‹œê°í™” ìƒì„± ì¤‘...")
    
    # 1. ì •ëŸ‰ ì§€í‘œ ë¹„êµ (ë§‰ëŒ€ê·¸ë˜í”„)
    metrics_df = pd.DataFrame({
        'Dataset 1': [res1['unique_tokens'], res1['ttr']],
        'Dataset 2': [res2['unique_tokens'], res2['ttr']]
    }, index=['ê³ ìœ  ì–´íœ˜ ìˆ˜', 'ì–´íœ˜ ë‹¤ì–‘ì„± (TTR)'])
    
    fig, ax = plt.subplots(1, 2, figsize=(12, 5))
    metrics_df.loc[['ê³ ìœ  ì–´íœ˜ ìˆ˜']].plot(kind='bar', ax=ax[0], title='ê³ ìœ  ì–´íœ˜ ìˆ˜ ë¹„êµ', rot=0)
    metrics_df.loc[['ì–´íœ˜ ë‹¤ì–‘ì„± (TTR)']].plot(kind='bar', ax=ax[1], title='ì–´íœ˜ ë‹¤ì–‘ì„±(TTR) ë¹„êµ', rot=0)
    plt.tight_layout()
    save_path = output_dir / "2a_lexical_metrics_comparison.png"
    plt.savefig(save_path)
    plt.close()
    
    # 2. ì›Œë“œ í´ë¼ìš°ë“œ
    font_path = "c:/Windows/Fonts/malgun.ttf" # ìœˆë„ìš° ë§‘ì€ ê³ ë”• ê¸°ì¤€
    wc = WordCloud(width=800, height=400, background_color='white', font_path=font_path)
    
    fig, axes = plt.subplots(2, 1, figsize=(12, 10))
    wc.generate_from_frequencies(res1['tfidf_scores_for_wc'])
    axes[0].imshow(wc, interpolation='bilinear')
    axes[0].set_title('Dataset 1 í•µì‹¬ í‚¤ì›Œë“œ (Word Cloud)')
    axes[0].axis('off')
    
    wc.generate_from_frequencies(res2['tfidf_scores_for_wc'])
    axes[1].imshow(wc, interpolation='bilinear')
    axes[1].set_title('Dataset 2 í•µì‹¬ í‚¤ì›Œë“œ (Word Cloud)')
    axes[1].axis('off')
    
    plt.tight_layout()
    save_path = output_dir / "2b_keyword_wordclouds.png"
    plt.savefig(save_path)
    plt.close()
    logging.info(f"ì‹œê°í™” ì €ì¥ ì™„ë£Œ: {save_path}")

def analyze_cosine_similarity(df1: pd.DataFrame, df2: pd.DataFrame, output_dir: Path):
    """ë°ì´í„°ì…‹ ë‚´/ê°„ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ë¶„í¬ë¥¼ ë¶„ì„í•˜ê³  ì‹œê°í™”í•©ë‹ˆë‹¤."""
    logging.info("ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ë¶„ì„ ì‹œì‘...")
    
    emb1 = np.array(df1['embedding'].tolist())
    emb2 = np.array(df2['embedding'].tolist())
    
    # ë°ì´í„°ì…‹ 1 ë‚´ë¶€ ìœ ì‚¬ë„
    sim_intra1 = cosine_similarity(emb1)
    intra1_scores = sim_intra1[np.triu_indices(len(emb1), k=1)]
    
    # ë°ì´í„°ì…‹ 2 ë‚´ë¶€ ìœ ì‚¬ë„
    sim_intra2 = cosine_similarity(emb2)
    intra2_scores = sim_intra2[np.triu_indices(len(emb2), k=1)]
    
    # ë°ì´í„°ì…‹ ê°„ ìœ ì‚¬ë„
    sim_inter = cosine_similarity(emb1, emb2)
    inter_scores = sim_inter.flatten()
    
    # ì‹œê°í™”
    sim_df = pd.concat([
        pd.DataFrame({'score': intra1_scores, 'type': 'Intra-Dataset 1'}),
        pd.DataFrame({'score': intra2_scores, 'type': 'Intra-Dataset 2'}),
        pd.DataFrame({'score': inter_scores, 'type': 'Inter-Dataset'})
    ])
    
    plt.figure(figsize=(10, 6))
    sns.violinplot(data=sim_df, x='type', y='score')
    plt.title('ì§ˆë¬¸ ì„ë² ë”© ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ë¶„í¬')
    plt.ylabel('Cosine Similarity')
    plt.xlabel('ë¹„êµ ìœ í˜•')
    
    save_path = output_dir / "3_cosine_similarity_distribution.png"
    plt.savefig(save_path)
    plt.close()
    logging.info(f"ì‹œê°í™” ì €ì¥ ì™„ë£Œ: {save_path}")
    
def visualize_t_sne(df1: pd.DataFrame, df2: pd.DataFrame, output_dir: Path):
    """t-SNEë¥¼ ì‚¬ìš©í•˜ì—¬ ì„ë² ë”© ê³µê°„ì„ 2Dë¡œ ì‹œê°í™”í•©ë‹ˆë‹¤."""
    logging.info("t-SNE ë¶„ì„ ì‹œì‘... (ì‹œê°„ì´ ë‹¤ì†Œ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
    
    emb1 = np.array(df1['embedding'].tolist())
    emb2 = np.array(df2['embedding'].tolist())
    
    # ë°ì´í„°ì…‹ í¬ê¸°ë¥¼ ì¤„ì—¬ì„œ t-SNE ì‹¤í–‰ (ìƒ˜í”Œë§)
    sample_size = min(2000, len(emb1), len(emb2))
    idx1 = np.random.choice(len(emb1), sample_size, replace=False)
    idx2 = np.random.choice(len(emb2), sample_size, replace=False)
    
    embeddings = np.vstack([emb1[idx1], emb2[idx2]])
    labels = np.array([0] * sample_size + [1] * sample_size)
    
    tsne = TSNE(n_components=2, perplexity=30, n_iter=1000, random_state=42, verbose=1)
    tsne_results = tsne.fit_transform(embeddings)
    
    tsne_df = pd.DataFrame({
        'x': tsne_results[:, 0],
        'y': tsne_results[:, 1],
        'label': ['Dataset 1' if l == 0 else 'Dataset 2' for l in labels]
    })
    
    plt.figure(figsize=(10, 8))
    sns.scatterplot(data=tsne_df, x='x', y='y', hue='label', alpha=0.7, s=20)
    plt.title('t-SNE ê¸°ë°˜ ì§ˆë¬¸ ì„ë² ë”© 2D ì‹œê°í™”')
    plt.xlabel('t-SNE Dimension 1')
    plt.ylabel('t-SNE Dimension 2')
    plt.legend(title='Dataset')
    
    save_path = output_dir / "4_tSNE_embedding_visualization.png"
    plt.savefig(save_path)
    plt.close()
    logging.info(f"ì‹œê°í™” ì €ì¥ ì™„ë£Œ: {save_path}")


# --- 2. GUI ì• í”Œë¦¬ì¼€ì´ì…˜ í´ë˜ìŠ¤ ---
class DataExplorerApp(ttk.Frame):
    def __init__(self, master):
        super().__init__(master, padding="10")
        self.master = master
        self.master.title("ë°ì´í„° íƒìƒ‰ ë° ë¶„ì„ ë„êµ¬ v1.0")
        self.master.geometry("800x600")
        self.grid(sticky=(tk.W, tk.E, tk.N, tk.S))
        self.master.columnconfigure(0, weight=1); self.master.rowconfigure(0, weight=1)

        self.file1_path = tk.StringVar()
        self.file2_path = tk.StringVar()
        self.status_text = tk.StringVar(value="ëŒ€ê¸° ì¤‘. ë¶„ì„í•  ë‘ ê°œì˜ .jsonl íŒŒì¼ì„ ì„ íƒí•´ì£¼ì„¸ìš”.")
        self.log_queue = queue.Queue()

        self.okt = Okt() # Okt ì´ˆê¸°í™”

        self.create_widgets()
        self.setup_logging()
        self.master.after(100, self.process_queue)

    def create_widgets(self):
        # íŒŒì¼ ì„ íƒ í”„ë ˆì„
        file_frame = ttk.LabelFrame(self, text="íŒŒì¼ ì„ íƒ", padding="10")
        file_frame.grid(row=0, column=0, sticky=(tk.W, tk.E), padx=5, pady=5)
        file_frame.columnconfigure(1, weight=1)

        ttk.Button(file_frame, text="Dataset 1 (.jsonl) ì„ íƒ", command=lambda: self.select_file(self.file1_path)).grid(row=0, column=0, padx=5, pady=5)
        ttk.Entry(file_frame, textvariable=self.file1_path, state='readonly').grid(row=0, column=1, sticky=(tk.W, tk.E), padx=5)

        ttk.Button(file_frame, text="Dataset 2 (.jsonl) ì„ íƒ", command=lambda: self.select_file(self.file2_path)).grid(row=1, column=0, padx=5, pady=5)
        ttk.Entry(file_frame, textvariable=self.file2_path, state='readonly').grid(row=1, column=1, sticky=(tk.W, tk.E), padx=5)

        # ì‹¤í–‰ í”„ë ˆì„
        action_frame = ttk.Frame(self, padding="10")
        action_frame.grid(row=1, column=0, sticky=tk.E, padx=5, pady=5)
        self.analyze_button = ttk.Button(action_frame, text="ë¶„ì„ ì‹œì‘", command=self.start_analysis, state='disabled')
        self.analyze_button.pack()

        # ìƒíƒœ ë° ë¡œê·¸ í”„ë ˆì„
        status_frame = ttk.LabelFrame(self, text="ì§„í–‰ ìƒíƒœ ë° ë¡œê·¸", padding="10")
        status_frame.grid(row=2, column=0, sticky=(tk.W, tk.E, tk.N, tk.S), padx=5, pady=5)
        self.rowconfigure(2, weight=1)
        status_frame.columnconfigure(0, weight=1); status_frame.rowconfigure(1, weight=1)

        ttk.Label(status_frame, textvariable=self.status_text).grid(row=0, column=0, sticky=(tk.W, tk.E))
        self.log_text = scrolledtext.ScrolledText(status_frame, state='disabled', wrap=tk.WORD, height=15)
        self.log_text.grid(row=1, column=0, sticky=(tk.W, tk.E, tk.N, tk.S), pady=5)

    def select_file(self, path_var):
        filepath = filedialog.askopenfilename(title="jsonl íŒŒì¼ì„ ì„ íƒí•˜ì„¸ìš”", filetypes=[("JSON Lines", "*.jsonl"), ("All files", "*.*")])
        if filepath:
            path_var.set(filepath)
            if self.file1_path.get() and self.file2_path.get():
                self.analyze_button.config(state='normal')

    def setup_logging(self):
        log_handler = QueueHandler(self.log_queue)
        logging.basicConfig(level=logging.INFO, format='%(asctime)s - [%(levelname)s] - %(message)s', datefmt='%H:%M:%S', handlers=[log_handler])

    def process_queue(self):
        try:
            while True: self.display_log(self.log_queue.get_nowait())
        except queue.Empty: pass
        finally: self.master.after(100, self.process_queue)

    def display_log(self, record):
        msg = logging.getLogger().handlers[0].format(record)
        self.log_text.configure(state='normal')
        self.log_text.insert(tk.END, msg + '\n')
        self.log_text.configure(state='disabled')
        self.log_text.yview(tk.END)

    def start_analysis(self):
        self.analyze_button.config(state='disabled')
        self.log_text.config(state='normal'); self.log_text.delete(1.0, tk.END); self.log_text.config(state='disabled')
        
        threading.Thread(target=self.run_full_analysis, daemon=True).start()

    def run_full_analysis(self):
        try:
            self.status_text.set("ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
            output_dir = Path(f"analysis_ê²°ê³¼_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}")
            output_dir.mkdir(exist_ok=True)
            logging.info(f"ê²°ê³¼ ì €ì¥ í´ë”: {output_dir}")

            # ë°ì´í„° ë¡œë”©
            self.status_text.set("Dataset 1 ë¡œë”© ì¤‘...")
            df1 = pd.read_json(self.file1_path.get(), lines=True)
            logging.info(f"Dataset 1 ë¡œë“œ ì™„ë£Œ: {len(df1)}ê°œ í•­ëª©")

            self.status_text.set("Dataset 2 ë¡œë”© ì¤‘...")
            df2 = pd.read_json(self.file2_path.get(), lines=True)
            logging.info(f"Dataset 2 ë¡œë“œ ì™„ë£Œ: {len(df2)}ê°œ í•­ëª©")

            # Phase 1: í…ìŠ¤íŠ¸ ë¶„ì„
            self.status_text.set("Phase 1: í…ìŠ¤íŠ¸ ê¸¸ì´ ë¶„ì„ ì¤‘...")
            df1_len = analyze_text_length(df1.copy(), "Dataset 1")
            df2_len = analyze_text_length(df2.copy(), "Dataset 2")
            visualize_length_distributions(df1_len, df2_len, output_dir)

            self.status_text.set("Phase 1: ì–´íœ˜ ë‹¤ì–‘ì„± ë¶„ì„ ì¤‘...")
            lex_res1 = analyze_lexical_diversity(df1, self.okt, "Dataset 1")
            lex_res2 = analyze_lexical_diversity(df2, self.okt, "Dataset 2")
            visualize_lexical_analysis(lex_res1, lex_res2, output_dir)

            # Phase 2: ì„ë² ë”© ë¶„ì„
            self.status_text.set("Phase 2: ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ë¶„ì„ ì¤‘...")
            analyze_cosine_similarity(df1, df2, output_dir)
            
            self.status_text.set("Phase 2: t-SNE 2D ì‹œê°í™” ì¤‘... (ì‹œê°„ ì†Œìš”)")
            visualize_t_sne(df1, df2, output_dir)

            self.status_text.set(f"ğŸ‰ ëª¨ë“  ë¶„ì„ ì™„ë£Œ! ê²°ê³¼ê°€ '{output_dir}' í´ë”ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
            logging.info("ëª¨ë“  ë¶„ì„ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        except Exception as e:
            logging.critical(f"ë¶„ì„ ì¤‘ ì¹˜ëª…ì ì¸ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
            self.status_text.set(f"ì˜¤ë¥˜ ë°œìƒ: {e}")
        finally:
            self.analyze_button.config(state='normal')

class QueueHandler(logging.Handler):
    def __init__(self, log_queue): super().__init__(); self.log_queue = log_queue
    def emit(self, record): self.log_queue.put(record)

# --- 3. ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹¤í–‰ ---
if __name__ == "__main__":
    root = tk.Tk()
    app = DataExplorerApp(master=root)
    app.mainloop()
