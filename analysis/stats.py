import tkinter as tk
from tkinter import filedialog, messagebox
import json
import os
import re
import datetime
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import nltk
from nltk.tokenize import word_tokenize, sent_tokenize # <<<<<<<<<<<<<<<< [ìˆ˜ì •ëœ ë¶€ë¶„] í•¨ìˆ˜ë¥¼ ì§ì ‘ ì‚¬ìš©í•˜ê¸° ìœ„í•´ ì¶”ê°€í•©ë‹ˆë‹¤.
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.manifold import TSNE
from wordcloud import WordCloud

# ==============================================================================
# 0. ì´ˆê¸° ì„¤ì • ë° í—¬í¼ í•¨ìˆ˜
# ==============================================================================

# NLTK í† í¬ë‚˜ì´ì € ë‹¤ìš´ë¡œë“œ (ìµœì´ˆ 1íšŒ ì‹¤í–‰)
try:
    nltk.data.find('tokenizers/punkt')
    nltk.data.find('tokenizers/punkt_tab')
except LookupError:
    print("NLTK 'punkt' í† í¬ë‚˜ì´ì €ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ìš´ë¡œë“œë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
    nltk.download('punkt')
    nltk.download('punkt_tab')
    print("ë‹¤ìš´ë¡œë“œ ì™„ë£Œ.")

# í•œê¸€ í°íŠ¸ ì„¤ì • (Windows: Malgun Gothic, Mac: AppleGothic)
try:
    plt.rc('font', family='Malgun Gothic')
    plt.rcParams['axes.unicode_minus'] = False 
    FONT_PATH = 'malgun.ttf'
except:
    try:
        plt.rc('font', family='AppleGothic')
        plt.rcParams['axes.unicode_minus'] = False
        FONT_PATH = '/System/Library/Fonts/Supplemental/AppleGothic.ttf'
    except:
        print("ê²½ê³ : 'Malgun Gothic' ë˜ëŠ” 'AppleGothic' í°íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì›Œë“œí´ë¼ìš°ë“œ ë° ì°¨íŠ¸ì˜ í•œê¸€ì´ ê¹¨ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        FONT_PATH = None

def clean_text(text: str, remove_choice_prefix: bool = False) -> str:
    """
    í…ìŠ¤íŠ¸ì—ì„œ ë¶ˆí•„ìš”í•œ ê³µë°±ê³¼ ì¤„ë°”ê¿ˆì„ ì œê±°í•˜ê³ ,
    í•„ìš” ì‹œ ë³´ê¸° ë²ˆí˜¸(1)Â·2) â€¦) ì ‘ë‘ì–´ë¥¼ ì‚­ì œí•©ë‹ˆë‹¤.

    Parameters
    ----------
    text : str
        ì›ë³¸ ë¬¸ìì—´
    remove_choice_prefix : bool, optional
        True ì´ë©´ '1) ' ~ '5) ' ê³¼ ê°™ì€ ë³´ê¸° ë²ˆí˜¸ ì ‘ë‘ì–´ë¥¼ ì œê±°í•©ë‹ˆë‹¤.
    """
    if not isinstance(text, str):
        return ""

    # ë³´ê¸° ë²ˆí˜¸ ì ‘ë‘ì–´ ì œê±° (ì„ íƒ)
    if remove_choice_prefix:
        text = re.sub(r'^\s*[1-5][)\.]?\s*', '', text)

    # HTML íƒœê·¸ ì œê±° (ì˜ˆ: <br>, <div> ë“±)
    text = re.sub(r'<[^>]+>', '', text)
    # ê´„í˜¸ ì•ˆ ë‚´ìš© ì œê±° (ì˜ˆ: "(ê·¸ë¦¼ ì°¸ì¡°)", "(ë³´ê¸°)")
    text = re.sub(r'\([^)]*\)', '', text)
    # ì¤„ë°”ê¿ˆ, íƒ­ ë“± ì œì–´ ë¬¸ì ì œê±°
    text = re.sub(r'[\r\n\t]', ' ', text)
    # íŠ¹ìˆ˜ ë¬¸ì ì •ë¦¬
    text = re.sub(r'[â€¢â—â–ªï¸]', ' ', text)
    # ì¤‘ë³µ ê³µë°± ì œê±° ë° ì–‘ ë ê³µë°± ì œê±°
    text = re.sub(r'\s+', ' ', text).strip()

    return text

# ----------------------------------------------------------------------
# ê°ê´€ì‹ë¬¸ì œ(5ì§€ì„ ë‹¤) ì „ìš© íŒŒì„œ
# ----------------------------------------------------------------------
def parse_json_questions(root_folder: str, status_updater=lambda msg: None):
    """
    ì—¬ëŸ¬ í´ë”ì— í©ì–´ì§„ JSON í˜•ì‹ 5ì§€ì„ ë‹¤ ë¬¸ì œë¥¼ í‘œì¤€ dict ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    ìœ íš¨ì„± ê²€ì‚¬:
      1) ë³´ê¸° 5ê°œ í•„ìˆ˜
      2) answer_idx ê°€ 0~4
    """
    records = []
    if not os.path.isdir(root_folder):
        status_updater(f"ì˜¤ë¥˜: í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {root_folder}")
        return records

    json_paths = [
        os.path.join(r, f)
        for r, _, fs in os.walk(root_folder)
        for f in fs if f.endswith('.json')
    ]
    choice_pat = re.compile(r'^\s*([1-5])\)\s*(.*)$')

    for p in json_paths:
        fname = os.path.basename(p)
        try:
            with open(p, 'r', encoding='utf-8-sig') as f:
                obj = json.load(f)
        except Exception as e:
            status_updater(f"ì˜¤ë¥˜: {fname} JSON ë¡œë“œ ì‹¤íŒ¨ â€“ {e}")
            continue

        q_raw = obj.get('question', '')
        a_raw = obj.get('answer', '')
        qa_id = obj.get('qa_id')
        domain = obj.get('domain')
        q_type = obj.get('q_type')

        if not q_raw or not a_raw:
            status_updater(f"ê²½ê³ : {fname} â€“ question/answer í•„ë“œ ëˆ„ë½. ê±´ë„ˆëœ€.")
            continue

        # question ì¤„ ë¶„ë¦¬
        stem_lines, choice_lines = [], []
        for ln in q_raw.splitlines():
            if choice_pat.match(ln):
                choice_lines.append(ln)
            else:
                stem_lines.append(ln)

        stem = clean_text(' '.join(stem_lines))

        # ë³´ê¸° ì¶”ì¶œ
        choices = [''] * 5
        for ln in choice_lines:
            m = choice_pat.match(ln)
            if m:
                idx = int(m.group(1)) - 1
                choices[idx] = clean_text(m.group(2), remove_choice_prefix=False)

        if '' in choices:
            status_updater(f"ì˜¤ë¥˜: {fname} â€“ 5ê°œ ë³´ê¸°ë¥¼ ëª¨ë‘ ì°¾ì§€ ëª»í•¨. ê±´ë„ˆëœ€.")
            continue

        # ë‹µì•ˆ íŒŒì‹±
        m_ans = choice_pat.match(a_raw)
        if not m_ans:
            status_updater(f"ì˜¤ë¥˜: {fname} â€“ answer í•„ë“œ í˜•ì‹ ë¶ˆì¼ì¹˜. ê±´ë„ˆëœ€.")
            continue
        answer_idx = int(m_ans.group(1)) - 1
        answer_text = clean_text(m_ans.group(2))

        # ìœ íš¨ì„± ê²€ì‚¬
        if len(choices) != 5:
            status_updater(f"ì˜¤ë¥˜: {fname} â€“ ë³´ê¸° ìˆ˜ 5ê°œ ì•„ë‹˜ ({len(choices)}ê°œ).")
            continue
        if not (0 <= answer_idx < 5):
            status_updater(f"ì˜¤ë¥˜: {fname} â€“ answer_idx ë²”ìœ„ ì˜¤ë¥˜ ({answer_idx}).")
            continue

        records.append({
            'qa_id': qa_id,
            'domain': domain,
            'q_type': q_type,
            'stem': stem,
            'choices': choices,
            'answer_idx': answer_idx,
            'answer_text': answer_text,
            'source_file': fname
        })
    return records


def parse_csv_questions(csv_path: str, status_updater=lambda msg: None):
    """
    CSV í˜•ì‹ 5ì§€ì„ ë‹¤ ë¬¸ì œë¥¼ í‘œì¤€ dict ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    í•„ìˆ˜ ì—´: question, A, B, C, D, E, answer
    """
    records = []
    if not os.path.isfile(csv_path):
        status_updater(f"ì˜¤ë¥˜: CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {csv_path}")
        return records

    try:
        df = pd.read_csv(csv_path, encoding='utf-8-sig')
    except Exception as e:
        status_updater(f"ì˜¤ë¥˜: CSV ë¡œë“œ ì‹¤íŒ¨ â€“ {e}")
        return records

    required_cols = ['question', 'A', 'B', 'C', 'D', 'E', 'answer']
    missing = [c for c in required_cols if c not in df.columns]
    if missing:
        status_updater(f"ì˜¤ë¥˜: CSV í•„ìˆ˜ ì—´ ëˆ„ë½ â€“ {missing}")
        return records

    for _, row in df.iterrows():
        stem = clean_text(str(row['question']))
        choices = [clean_text(str(row[c]), remove_choice_prefix=True) for c in ['A', 'B', 'C', 'D', 'E']]
        answer_idx = int(row['answer']) - 1

        # ìœ íš¨ì„± ê²€ì‚¬
        if len(choices) != 5:
            status_updater("ì˜¤ë¥˜: CSV â€“ ë³´ê¸° ìˆ˜ 5ê°œ ì•„ë‹˜.")
            continue
        if not (0 <= answer_idx < 5):
            status_updater("ì˜¤ë¥˜: CSV â€“ answer_idx ë²”ìœ„ ì˜¤ë¥˜.")
            continue

        records.append({
            'qa_id': row.get('qa_id', None),
            'domain': row.get('domain', None),
            'q_type': row.get('q_type', 1),
            'stem': stem,
            'choices': choices,
            'answer_idx': answer_idx,
            'answer_text': choices[answer_idx],
            'source_file': os.path.basename(csv_path)
        })
    return records


def records_to_dataframe(records: list) -> pd.DataFrame:
    """í‘œì¤€ dict ë¦¬ìŠ¤íŠ¸ë¥¼ DataFrame ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    return pd.DataFrame(records)


def load_and_parse_json_data(folder_path, status_updater):
    """
    ì§€ì •ëœ í´ë”ì—ì„œ ëª¨ë“  JSON íŒŒì¼ì„ ì¬ê·€ì ìœ¼ë¡œ íƒìƒ‰í•˜ê³  íŒŒì‹±í•©ë‹ˆë‹¤.
    """
    data_list = []
    if not os.path.isdir(folder_path):
        status_updater(f"ì˜¤ë¥˜: í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {folder_path}")
        return []

    file_paths = [os.path.join(root, file) for root, _, files in os.walk(folder_path) for file in files if file.endswith('.json')]
    
    for i, file_path in enumerate(file_paths):
        file_name = os.path.basename(file_path)
        if (i + 1) % 50 == 0:
             status_updater(f"íŒŒì¼ ë¡œë“œ ì¤‘... ({i+1}/{len(file_paths)})")
        try:
            with open(file_path, 'r', encoding='utf-8-sig') as f: # json encoding ë³€ê²½
                data = json.load(f)
                if data.get("q_type") != 1: # ê°ê´€ì‹ ë¬¸ì œë¡œë§Œ ë¹„êµ
                    continue
                text_content = None
                embedding_vector = None

                if "original_text" in data and "embedding" in data:
                    text_content = data.get("original_text")
                    embedding_vector = data.get("embedding")
                elif "text" in data and "embeddings" in data:
                    text_content = data.get("text")
                    embedding_vector = data.get("embeddings")

                if text_content and embedding_vector is not None and len(embedding_vector) > 0:
                    data_list.append({
                        'filename': file_name,
                        'text': clean_text(text_content),
                        'embedding': np.array(embedding_vector, dtype=float).flatten() 
                    })
                else:
                    status_updater(f"ê²½ê³ : {file_name}ì—ì„œ ìœ íš¨í•œ í…ìŠ¤íŠ¸ ë˜ëŠ” ì„ë² ë”©ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤.")
        except json.JSONDecodeError:
            status_updater(f"ì˜¤ë¥˜: {file_name} íŒŒì¼ì´ ìœ íš¨í•œ JSON í˜•ì‹ì´ ì•„ë‹™ë‹ˆë‹¤.")
        except Exception as e:
            status_updater(f"ì˜¤ë¥˜: {file_name} íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")
    return data_list

# ==============================================================================
# Phase 1: ë°ì´í„° ê¸°ì´ˆ íŠ¹ì„± ë¶„ì„
# ==============================================================================

def analyze_text_lengths(data_group_1, data_group_2, output_dir, status_updater):
    """1) í…ìŠ¤íŠ¸ ê¸¸ì´ ë¶„í¬ ë¶„ì„"""
    status_updater("\n[1ë‹¨ê³„] í…ìŠ¤íŠ¸ ê¸¸ì´ ë¶„í¬ ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    
    def get_stats(data, group_name):
        return pd.DataFrame([
            {
                'ê¸€ì ìˆ˜': len(d['text']),
                'ë‹¨ì–´ ìˆ˜': len(word_tokenize(d['text'])),
                'ë¬¸ì¥ ìˆ˜': len(sent_tokenize(d['text'])),
                'ê·¸ë£¹': group_name
            } for d in data if d['text']
        ])

    df_len_g1 = get_stats(data_group_1, 'ê·¸ë£¹ 1')
    df_len_g2 = get_stats(data_group_2, 'ê·¸ë£¹ 2')
    df_lengths = pd.concat([df_len_g1, df_len_g2])

    status_updater("\n[ê·¸ë£¹ 1] í…ìŠ¤íŠ¸ ê¸¸ì´ ê¸°ìˆ  í†µê³„ëŸ‰:")
    status_updater(df_len_g1.describe().to_string())
    status_updater("\n[ê·¸ë£¹ 2] í…ìŠ¤íŠ¸ ê¸¸ì´ ê¸°ìˆ  í†µê³„ëŸ‰:")
    status_updater(df_len_g2.describe().to_string())

    length_metrics = ['ê¸€ì ìˆ˜', 'ë‹¨ì–´ ìˆ˜', 'ë¬¸ì¥ ìˆ˜']
    for metric in length_metrics:
        plt.figure(figsize=(12, 7))
        sns.histplot(data=df_lengths, x=metric, hue='ê·¸ë£¹', kde=True, palette='viridis', alpha=0.6, multiple='stack')
        plt.title(f'ğŸ“Š ê·¸ë£¹ë³„ {metric} ë¶„í¬ ë¹„êµ (íˆìŠ¤í† ê·¸ë¨)', fontsize=16)
        plt.savefig(os.path.join(output_dir, f'1_length_{metric}_histogram.png'))
        plt.close()

        plt.figure(figsize=(10, 7))
        sns.violinplot(data=df_lengths, x='ê·¸ë£¹', y=metric, palette='viridis', inner='quartile')
        plt.title(f'ğŸ» ê·¸ë£¹ë³„ {metric} ë¶„í¬ ë¹„êµ (ë°”ì´ì˜¬ë¦° í”Œë¡¯)', fontsize=16)
        plt.savefig(os.path.join(output_dir, f'1_length_{metric}_violinplot.png'))
        plt.close()

    status_updater("[1ë‹¨ê³„] ë¶„ì„ ì™„ë£Œ. ê²°ê³¼ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

def analyze_vocabulary_and_keywords(data_group_1, data_group_2, output_dir, status_updater):
    """2) ì–´íœ˜ ë‹¤ì–‘ì„± ë° í•µì‹¬ í‚¤ì›Œë“œ ë¶„ì„"""
    status_updater("\n[2ë‹¨ê³„] ì–´íœ˜ ë‹¤ì–‘ì„± ë° í•µì‹¬ í‚¤ì›Œë“œ ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    results = {}
    
    for i, (group_data, group_name) in enumerate([(data_group_1, "ê·¸ë£¹ 1"), (data_group_2, "ê·¸ë£¹ 2")]):
        all_texts = [d['text'] for d in group_data if d['text']]
        if not all_texts:
            status_updater(f"ê²½ê³ : {group_name}ì— ë¶„ì„í•  í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
            continue
            
        all_tokens = [word for text in all_texts for word in word_tokenize(text)]
        unique_words = set(all_tokens)
        ttr = len(unique_words) / len(all_tokens) if all_tokens else 0
        
        results[group_name] = {'ê³ ìœ  ì–´íœ˜ ìˆ˜': len(unique_words), 'TTR': ttr}
        status_updater(f"\n[{group_name}] ì–´íœ˜ ë‹¤ì–‘ì„±: ê³ ìœ  ì–´íœ˜ ìˆ˜={len(unique_words)}, TTR={ttr:.4f}")

        try:
            vectorizer = TfidfVectorizer(max_features=2000, tokenizer=word_tokenize, stop_words=None)
            tfidf_matrix = vectorizer.fit_transform(all_texts)
            feature_names = vectorizer.get_feature_names_out()
            avg_tfidf_scores = tfidf_matrix.mean(axis=0).A1
            
            top_indices = avg_tfidf_scores.argsort()[::-1][:20]
            top_keywords = {feature_names[i]: avg_tfidf_scores[i] for i in top_indices}
            
            keywords_df = pd.DataFrame(list(top_keywords.items()), columns=['í‚¤ì›Œë“œ', 'í‰ê·  TF-IDF'])
            status_updater(f"[{group_name}] ìƒìœ„ TF-IDF í‚¤ì›Œë“œ:")
            status_updater(keywords_df.to_string(index=False))
            keywords_df.to_csv(os.path.join(output_dir, f'2_{group_name}_keywords.csv'), index=False, encoding='utf-8-sig')

            if top_keywords and FONT_PATH:
                wc = WordCloud(font_path=FONT_PATH, background_color='white', width=800, height=400, collocations=False).generate_from_frequencies(top_keywords)
                plt.figure(figsize=(10, 5))
                plt.imshow(wc, interpolation='bilinear')
                plt.title(f'â˜ï¸ [{group_name}] í•µì‹¬ í‚¤ì›Œë“œ ì›Œë“œí´ë¼ìš°ë“œ', fontsize=16)
                plt.axis('off')
                plt.savefig(os.path.join(output_dir, f'2_{group_name}_wordcloud.png'))
                plt.close()
        except Exception as e:
            status_updater(f"ê²½ê³ : [{group_name}] TF-IDF ë˜ëŠ” ì›Œë“œí´ë¼ìš°ë“œ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")

    if results:
        df_metrics = pd.DataFrame.from_dict(results, orient='index').reset_index().rename(columns={'index': 'ê·¸ë£¹'})
        df_melted = df_metrics.melt(id_vars='ê·¸ë£¹', var_name='ì§€í‘œ', value_name='ê°’')
        plt.figure(figsize=(12, 7))
        sns.barplot(data=df_melted, x='ì§€í‘œ', y='ê°’', hue='ê·¸ë£¹', palette='coolwarm')
        plt.title('ğŸ“Š ê·¸ë£¹ë³„ ì–´íœ˜ ë‹¤ì–‘ì„± ì§€í‘œ ë¹„êµ', fontsize=16)
        plt.yscale('log')
        plt.ylabel('ê°’ (ë¡œê·¸ ìŠ¤ì¼€ì¼)')
        plt.savefig(os.path.join(output_dir, '2_vocab_diversity_barchart.png'))
        plt.close()

    status_updater("[2ë‹¨ê³„] ë¶„ì„ ì™„ë£Œ. ê²°ê³¼ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")


# ==============================================================================
# Phase 2: ì„ë² ë”© ê¸°ë°˜ ì˜ë¯¸ ê³µê°„ ë¶„ì„
# ==============================================================================

def analyze_cosine_similarity(data_group_1, data_group_2, output_dir, status_updater, sample_size=500):
    """3) ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ë¶„í¬ ë¶„ì„"""
    status_updater("\n[3ë‹¨ê³„] ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    
    embeddings_g1 = np.array([d['embedding'] for d in data_group_1 if 'embedding' in d and len(d['embedding']) > 0])
    embeddings_g2 = np.array([d['embedding'] for d in data_group_2 if 'embedding' in d and len(d['embedding']) > 0])

    if len(embeddings_g1) < 2 or len(embeddings_g2) < 2:
        status_updater("ì˜¤ë¥˜: ìœ ì‚¬ë„ ë¶„ì„ì„ ìœ„í•´ ê° ê·¸ë£¹ì— ìµœì†Œ 2ê°œ ì´ìƒì˜ ìœ íš¨í•œ ì„ë² ë”©ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        return

    # ë°ì´í„°ê°€ ë„ˆë¬´ í´ ê²½ìš° ìƒ˜í”Œë§
    if len(embeddings_g1) > sample_size:
        status_updater(f"ê·¸ë£¹ 1ì´ ë„ˆë¬´ ì»¤ì„œ({len(embeddings_g1)}ê°œ) {sample_size}ê°œë¡œ ìƒ˜í”Œë§í•©ë‹ˆë‹¤.")
        indices = np.random.choice(len(embeddings_g1), sample_size, replace=False)
        embeddings_g1 = embeddings_g1[indices]
    if len(embeddings_g2) > sample_size:
        status_updater(f"ê·¸ë£¹ 2ê°€ ë„ˆë¬´ ì»¤ì„œ({len(embeddings_g2)}ê°œ) {sample_size}ê°œë¡œ ìƒ˜í”Œë§í•©ë‹ˆë‹¤.")
        indices = np.random.choice(len(embeddings_g2), sample_size, replace=False)
        embeddings_g2 = embeddings_g2[indices]
        
    sim_data = []

    # ê·¸ë£¹ 1 ë‚´ë¶€ ìœ ì‚¬ë„
    sim_matrix_g1 = cosine_similarity(embeddings_g1)
    intra_sim_g1 = sim_matrix_g1[np.triu_indices_from(sim_matrix_g1, k=1)]
    sim_data.extend([(s, 'ê·¸ë£¹ 1 ë‚´ë¶€') for s in intra_sim_g1])
    
    # ê·¸ë£¹ 2 ë‚´ë¶€ ìœ ì‚¬ë„
    sim_matrix_g2 = cosine_similarity(embeddings_g2)
    intra_sim_g2 = sim_matrix_g2[np.triu_indices_from(sim_matrix_g2, k=1)]
    sim_data.extend([(s, 'ê·¸ë£¹ 2 ë‚´ë¶€') for s in intra_sim_g2])

    # ê·¸ë£¹ ê°„ ìœ ì‚¬ë„
    inter_sim = cosine_similarity(embeddings_g1, embeddings_g2).flatten()
    sim_data.extend([(s, 'ê·¸ë£¹ ê°„') for s in inter_sim])

    df_sim = pd.DataFrame(sim_data, columns=['ì½”ì‚¬ì¸ ìœ ì‚¬ë„', 'ë¹„êµ ìœ í˜•'])

    status_updater(f"\nê·¸ë£¹ 1 ë‚´ë¶€ í‰ê·  ìœ ì‚¬ë„: {np.mean(intra_sim_g1):.4f}")
    status_updater(f"ê·¸ë£¹ 2 ë‚´ë¶€ í‰ê·  ìœ ì‚¬ë„: {np.mean(intra_sim_g2):.4f}")
    status_updater(f"ê·¸ë£¹ ê°„ í‰ê·  ìœ ì‚¬ë„: {np.mean(inter_sim):.4f}")
    
    plt.figure(figsize=(12, 8))
    sns.violinplot(data=df_sim, x='ë¹„êµ ìœ í˜•', y='ì½”ì‚¬ì¸ ìœ ì‚¬ë„', palette='plasma')
    plt.title('ğŸ» ê·¸ë£¹ ë‚´/ê°„ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ë¶„í¬', fontsize=16)
    plt.savefig(os.path.join(output_dir, '3_cosine_similarity_violinplot.png'))
    plt.close()

    status_updater("[3ë‹¨ê³„] ë¶„ì„ ì™„ë£Œ. ê²°ê³¼ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

def analyze_embedding_clusters_tsne(data_group_1, data_group_2, output_dir, status_updater):
    """4) 2D ì„ë² ë”© ê³µê°„ ì‹œê°í™” (t-SNE)"""
    status_updater("\n[4ë‹¨ê³„] t-SNE ì„ë² ë”© ê³µê°„ ì‹œê°í™”ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")

    embeddings_g1 = np.array([d['embedding'] for d in data_group_1 if 'embedding' in d and len(d['embedding']) > 0])
    embeddings_g2 = np.array([d['embedding'] for d in data_group_2 if 'embedding' in d and len(d['embedding']) > 0])

    if len(embeddings_g1) == 0 or len(embeddings_g2) == 0:
        status_updater("ì˜¤ë¥˜: t-SNE ë¶„ì„ì„ ìœ„í•´ ê° ê·¸ë£¹ì— ìµœì†Œ 1ê°œ ì´ìƒì˜ ìœ íš¨í•œ ì„ë² ë”©ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        return

    all_embeddings = np.vstack([embeddings_g1, embeddings_g2])
    labels = ['ê·¸ë£¹ 1'] * len(embeddings_g1) + ['ê·¸ë£¹ 2'] * len(embeddings_g2)

    if all_embeddings.shape[1] <= 2:
        status_updater("ê²½ê³ : ì„ë² ë”© ì°¨ì›ì´ ì´ë¯¸ 2ì°¨ì› ì´í•˜ì…ë‹ˆë‹¤. t-SNEë¥¼ ê±´ë„ˆë›°ê³  ë°”ë¡œ ì‹œê°í™”í•©ë‹ˆë‹¤.")
        tsne_results = all_embeddings
    else:
        perplexity = min(30, len(all_embeddings) - 2)
        if perplexity < 5:
            status_updater(f"ê²½ê³ : ë°ì´í„° í¬ì¸íŠ¸({len(all_embeddings)}ê°œ)ê°€ ë„ˆë¬´ ì ì–´ t-SNE ë¶„ì„ì´ ë¶ˆì•ˆì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            perplexity = max(1, len(all_embeddings) - 2)

        tsne = TSNE(n_components=2, perplexity=perplexity, random_state=42, n_iter=1000, learning_rate='auto', init='pca')
        tsne_results = tsne.fit_transform(all_embeddings)

    df_tsne = pd.DataFrame({'x': tsne_results[:, 0], 'y': tsne_results[:, 1], 'ê·¸ë£¹': labels})

    plt.figure(figsize=(12, 10))
    sns.scatterplot(
        data=df_tsne,
        x='x', y='y',
        hue='ê·¸ë£¹',
        palette='bright',
        alpha=0.7,
        s=50
    )
    plt.title('ğŸ—ºï¸ t-SNE ê¸°ë°˜ 2D ì„ë² ë”© ê³µê°„ ì‹œê°í™”', fontsize=16)
    plt.legend(title='ë°ì´í„° ê·¸ë£¹')
    plt.xlabel('t-SNE Component 1')
    plt.ylabel('t-SNE Component 2')
    plt.grid(True)
    plt.savefig(os.path.join(output_dir, '4_tsne_embedding_space.png'))
    plt.close()
    
    status_updater("[4ë‹¨ê³„] ë¶„ì„ ì™„ë£Œ. ê²°ê³¼ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")


# ==============================================================================
# GUI ë° ë©”ì¸ ì‹¤í–‰ ë¡œì§
# ==============================================================================

class NLPAnalysisApp:
    def __init__(self, master):
        self.master = master
        master.title("í…ìŠ¤íŠ¸ ë¹„êµ ë¶„ì„ ë„êµ¬ (v1.2 - ì˜¤ë¥˜ ìˆ˜ì •)")
        master.geometry("700x550")

        self.folder_path1 = tk.StringVar()
        self.folder_path2 = tk.StringVar()
        
        self.create_widgets()

    def create_widgets(self):
        main_frame = tk.Frame(self.master, padx=10, pady=10)
        main_frame.pack(fill=tk.BOTH, expand=True)
        
        folder_frame = tk.LabelFrame(main_frame, text="ì…ë ¥ ë°ì´í„° ì„¤ì •", padx=10, pady=10)
        folder_frame.pack(fill=tk.X, pady=5)
        
        tk.Label(folder_frame, text="ê·¸ë£¹ 1 ë°ì´í„° í´ë”:").grid(row=0, column=0, sticky='w', pady=2)
        tk.Entry(folder_frame, textvariable=self.folder_path1, width=60).grid(row=1, column=0, sticky='we')
        tk.Button(folder_frame, text="í´ë” ì„ íƒ", command=lambda: self.select_folder(self.folder_path1)).grid(row=1, column=1, padx=5)

        tk.Label(folder_frame, text="ê·¸ë£¹ 2 ë°ì´í„° í´ë”:").grid(row=2, column=0, sticky='w', pady=2)
        tk.Entry(folder_frame, textvariable=self.folder_path2, width=60).grid(row=3, column=0, sticky='we')
        tk.Button(folder_frame, text="í´ë” ì„ íƒ", command=lambda: self.select_folder(self.folder_path2)).grid(row=3, column=1, padx=5)

        folder_frame.grid_columnconfigure(0, weight=1)

        self.run_button = tk.Button(main_frame, text="ë¶„ì„ ì‹¤í–‰", command=self.run_analysis, font=("Helvetica", 12, "bold"), bg="#4CAF50", fg="white")
        self.run_button.pack(pady=10, fill=tk.X)

        status_frame = tk.LabelFrame(main_frame, text="ë¶„ì„ ë¡œê·¸", padx=10, pady=10)
        status_frame.pack(fill=tk.BOTH, expand=True)

        self.status_text = tk.Text(status_frame, height=15, state='disabled', wrap='word', bg="#f0f0f0")
        scrollbar = tk.Scrollbar(status_frame, command=self.status_text.yview)
        self.status_text.config(yscrollcommand=scrollbar.set)
        
        self.status_text.pack(side=tk.LEFT, fill=tk.BOTH, expand=True)
        scrollbar.pack(side=tk.RIGHT, fill=tk.Y)
        
        self.update_status("ë¶„ì„ ì¤€ë¹„ ì™„ë£Œ. ë‘ ê°œì˜ ë°ì´í„° í´ë”ë¥¼ ì„ íƒí•˜ê³  'ë¶„ì„ ì‹¤í–‰' ë²„íŠ¼ì„ ëˆ„ë¥´ì„¸ìš”.")

    def select_folder(self, entry_var):
        folder_selected = filedialog.askdirectory()
        if folder_selected:
            entry_var.set(folder_selected)
            self.update_status(f"í´ë” ì„ íƒë¨: {folder_selected}")

    def update_status(self, message):
        self.status_text.config(state='normal')
        self.status_text.insert(tk.END, message + "\n")
        self.status_text.see(tk.END)
        self.status_text.config(state='disabled')
        self.master.update_idletasks()

    def run_analysis(self):
        folder1 = self.folder_path1.get()
        folder2 = self.folder_path2.get()

        if not folder1 or not folder2:
            messagebox.showwarning("ì…ë ¥ ì˜¤ë¥˜", "ë‘ ê°œì˜ ë°ì´í„° í´ë”ë¥¼ ëª¨ë‘ ì„ íƒí•´ì£¼ì„¸ìš”.")
            return

        self.run_button.config(state='disabled', text="ë¶„ì„ ì¤‘...")
        try:
            timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
            output_dir = f"analysis_results_{timestamp}"
            os.makedirs(output_dir, exist_ok=True)

            self.update_status("="*50)
            self.update_status(f"ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤. ê²°ê³¼ëŠ” '{output_dir}' í´ë”ì— ì €ì¥ë©ë‹ˆë‹¤.")
            self.update_status("="*50)
            
            self.update_status("\n[ë°ì´í„° ë¡œë”©] ê·¸ë£¹ 1 ë°ì´í„° ë¡œë“œë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
            data_group_1 = load_and_parse_json_data(folder1, self.update_status)
            self.update_status(f"ê·¸ë£¹ 1ì—ì„œ {len(data_group_1)}ê°œ íŒŒì¼ ë¡œë“œ ì™„ë£Œ.")
            
            self.update_status("\n[ë°ì´í„° ë¡œë”©] ê·¸ë£¹ 2 ë°ì´í„° ë¡œë“œë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
            data_group_2 = load_and_parse_json_data(folder2, self.update_status)
            self.update_status(f"ê·¸ë£¹ 2ì—ì„œ {len(data_group_2)}ê°œ íŒŒì¼ ë¡œë“œ ì™„ë£Œ.")

            if not data_group_1 or not data_group_2:
                messagebox.showerror("ë°ì´í„° ì˜¤ë¥˜", "í•˜ë‚˜ ë˜ëŠ” ë‘ ê·¸ë£¹ ëª¨ë‘ì—ì„œ ìœ íš¨í•œ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. í´ë” ê²½ë¡œì™€ JSON íŒŒì¼ í˜•ì‹ì„ í™•ì¸í•˜ì„¸ìš”.")
                raise ValueError("ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨")

            analyze_text_lengths(data_group_1, data_group_2, output_dir, self.update_status)
            analyze_vocabulary_and_keywords(data_group_1, data_group_2, output_dir, self.update_status)
            analyze_cosine_similarity(data_group_1, data_group_2, output_dir, self.update_status)
            analyze_embedding_clusters_tsne(data_group_1, data_group_2, output_dir, self.update_status)

            self.update_status("\n" + "="*50)
            self.update_status("ëª¨ë“  ë¶„ì„ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
            self.update_status(f"ê²°ê³¼ë¬¼ì€ '{os.path.abspath(output_dir)}' í´ë”ì—ì„œ í™•ì¸í•˜ì„¸ìš”.")
            self.update_status("="*50)
            messagebox.showinfo("ë¶„ì„ ì™„ë£Œ", f"ëª¨ë“  ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ê²°ê³¼ë¥¼ '{output_dir}' í´ë”ì—ì„œ í™•ì¸í•˜ì„¸ìš”.")

        except Exception as e:
            self.update_status(f"\nì¹˜ëª…ì ì¸ ì˜¤ë¥˜ ë°œìƒ: {e}")
            messagebox.showerror("ë¶„ì„ ì˜¤ë¥˜", f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        finally:
            self.run_button.config(state='normal', text="ë¶„ì„ ì‹¤í–‰")

if __name__ == "__main__":
    root = tk.Tk()
    app = NLPAnalysisApp(root)
    root.mainloop()
