# generation/dynamic_question_generator.py
"""
ë²¡í„°DB ê¸°ë°˜ ë™ì  ì˜ë£Œ ë¬¸ì œ ìƒì„± ì‹œìŠ¤í…œ
ì‚¬ìš©ì ì¿¼ë¦¬ â†’ ë²¡í„° ê²€ìƒ‰ â†’ LLM ììœ¨ íŒë‹¨ â†’ ë¬¸ì œ ìƒì„±
"""
from pathlib import Path
import os
import json
from datetime import datetime
from typing import Dict, List, Any, Optional
from dataclasses import dataclass

# Phase IV ê²€ìƒ‰ ì—”ì§„
from search.search_engine import SearchEngine

# Gemini API
try:
    import google.generativeai as genai
    GEMINI_AVAILABLE = True
except ImportError:
    GEMINI_AVAILABLE = False

@dataclass
class SearchContext:
    query: str
    text_content: str
    image_info: str
    confidence: str
    has_images: bool
    estimated_topic: str
    primary_image_path: str = ""  # ì¶”ê°€
    
class DynamicQuestionGenerator:
    """ë²¡í„°DB ê¸°ë°˜ ë™ì  ë¬¸ì œ ìƒì„±ê¸°"""
    
    def __init__(self):
        print("ğŸ¤– ë™ì  ë¬¸ì œ ìƒì„± ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
        
        # ê²€ìƒ‰ ì—”ì§„ ì—°ê²°
        try:
            self.search_engine = SearchEngine()
            print("âœ… ë²¡í„°DB ê²€ìƒ‰ ì—”ì§„ ì—°ê²° ì™„ë£Œ")
        except Exception as e:
            print(f"âŒ ê²€ìƒ‰ ì—”ì§„ ì—°ê²° ì‹¤íŒ¨: {e}")
            self.search_engine = None
        
        # Gemini í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self.gemini_client = self._init_gemini()
        
        print("âœ… ë™ì  ë¬¸ì œ ìƒì„± ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ")
    
    def _init_gemini(self) -> Optional[object]:
        """Gemini API ì´ˆê¸°í™”"""
        api_key = os.getenv("GEMINI_API_KEY")
        if not api_key or not GEMINI_AVAILABLE:
            print("âš ï¸ Gemini API ì‚¬ìš© ë¶ˆê°€")
            return None
        
        try:
            genai.configure(api_key=api_key)
            model = genai.GenerativeModel('gemini-1.5-pro')
            print("âœ… Gemini 1.5 Pro ì—°ê²° ì™„ë£Œ")
            return model
        except Exception as e:
            print(f"âŒ Gemini ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return None
    
    def generate_question_from_query(self, user_query: str, top_k: int = 8) -> Dict[str, Any]:
        """ì‚¬ìš©ì ì¿¼ë¦¬ë¡œë¶€í„° ë™ì  ë¬¸ì œ ìƒì„±
        
        ì›Œí¬í”Œë¡œìš°:
        1. ì‚¬ìš©ì ì¿¼ë¦¬ â†’ ë²¡í„°DB ê²€ìƒ‰
        2. ê²€ìƒ‰ ê²°ê³¼ â†’ êµ¬ì¡°í™”ëœ ì»¨í…ìŠ¤íŠ¸
        3. ì»¨í…ìŠ¤íŠ¸ â†’ LLM ììœ¨ ë¶„ì„
        4. LLM â†’ ì ì ˆí•œ ë¬¸ì œ ìƒì„±
        """
        
        print(f"\nğŸ” ë™ì  ë¬¸ì œ ìƒì„±: '{user_query}'")
        print("="*60)
        
        try:
            # 1. ë²¡í„°DBì—ì„œ ê´€ë ¨ ì •ë³´ ê²€ìƒ‰
            print("1ï¸âƒ£ ë²¡í„°DB ê²€ìƒ‰ ì¤‘...")
            search_result = self.search_engine.search_text(user_query, top_k=top_k)
            
            if "error" in search_result:
                return {"error": f"ë²¡í„° ê²€ìƒ‰ ì‹¤íŒ¨: {search_result['error']}"}
            
            # 2. ê²€ìƒ‰ ê²°ê³¼ë¥¼ êµ¬ì¡°í™”ëœ ì»¨í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
            print("2ï¸âƒ£ ì»¨í…ìŠ¤íŠ¸ êµ¬ì¡°í™” ì¤‘...")
            context = self._create_search_context(search_result, user_query)
            
            # 3. LLMì—ê²Œ ì»¨í…ìŠ¤íŠ¸ ë¶„ì„ ë° ë¬¸ì œ ìƒì„± ìš”ì²­
            print("3ï¸âƒ£ LLM ë¬¸ì œ ìƒì„± ì¤‘...")
            if not self.gemini_client:
                return {"error": "Gemini API ì‚¬ìš© ë¶ˆê°€"}
            
            generated_question = self._generate_with_llm_analysis(context)
            
            if not generated_question or "error" in generated_question:
                return {"error": "LLM ë¬¸ì œ ìƒì„± ì‹¤íŒ¨"}
            
            # 4. ê²°ê³¼ êµ¬ì„±
            result = {
                "generated_question": generated_question,
                "search_context": {
                    "original_query": user_query,
                    "estimated_topic": context.estimated_topic,
                    "has_images": context.has_images,
                    "confidence": context.confidence,
                    "text_sources": search_result.get("text_count", 0),
                    "image_sources": search_result.get("image_count", 0),
                    "primary_image_path": context.primary_image_path
                },
                "generation_metadata": {
                    "method": "dynamic_vector_search",
                    "search_quality": context.confidence,
                    "llm_model": "gemini-1.5-pro",
                    "vector_db_used": True
                },
                "created_at": datetime.now().isoformat()
            }
            
            print(f"âœ… ë™ì  ë¬¸ì œ ìƒì„± ì™„ë£Œ: {context.estimated_topic}")
            return result
            
        except Exception as e:
            print(f"âŒ ë¬¸ì œ ìƒì„± ì‹¤íŒ¨: {e}")
            return {"error": f"ë™ì  ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}"}
    
    # dynamic_question_generator.py ìˆ˜ì • - ì´ë¯¸ì§€ ê²½ë¡œ ì§ì ‘ í¬í•¨

    def _create_search_context(self, search_result: Dict, user_query: str) -> SearchContext:
        """ê²€ìƒ‰ ê²°ê³¼ë¥¼ êµ¬ì¡°í™”ëœ ì»¨í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ (ì´ë¯¸ì§€ ê²½ë¡œ í¬í•¨)"""
        
        # ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
        text_content = search_result.get("text_content", "")
        image_info = search_result.get("image_info", "")
        confidence = search_result.get("confidence", "low")
        
        # ì´ë¯¸ì§€ ê²°ê³¼ì—ì„œ ì‹¤ì œ ê²½ë¡œ ì¶”ì¶œ
        images = search_result.get("images", [])
        has_images = len(images) > 0
        
        # ì²« ë²ˆì§¸ ì´ë¯¸ì§€ì˜ ê²½ë¡œ ì¶”ì¶œ
        primary_image_path = ""
        if images and isinstance(images, list):
            first_image = images[0]
            if isinstance(first_image, dict):
                primary_image_path = first_image.get("image_path", "")
        
        estimated_topic = search_result.get("korean_diagnosis", "ë¯¸ìƒ") or search_result.get("diagnosis", "Unknown")
        
        context = SearchContext(
            query=user_query,
            text_content=text_content,
            image_info=image_info,
            confidence=confidence,
            has_images=has_images,
            estimated_topic=estimated_topic,
            primary_image_path=primary_image_path  # ì¶”ê°€
        )
        
        print(f"   ğŸ“Š ì»¨í…ìŠ¤íŠ¸ ìƒì„±: {context.estimated_topic} (ì‹ ë¢°ë„: {confidence})")
        print(f"   ğŸ–¼ï¸ ì´ë¯¸ì§€ í¬í•¨: {has_images}")
        if primary_image_path:
            print(f"   ğŸ“· ì´ë¯¸ì§€ ê²½ë¡œ: {Path(primary_image_path).name}")
        
        return context

    
    def _generate_with_llm_analysis(self, context: SearchContext) -> Optional[Dict]:
        """LLMì´ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•˜ê³  ì ì ˆí•œ ë¬¸ì œ ìƒì„±"""
        
        # ë™ì  í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = self._create_dynamic_prompt(context)
        
        try:
            response = self.gemini_client.generate_content(prompt)
            
            # JSON ì¶”ì¶œ
            import re
            json_match = re.search(r'\{.*\}', response.text, re.DOTALL)
            
            if json_match:
                return json.loads(json_match.group())
            else:
                print("   âš ï¸ LLMì´ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì§€ ì•ŠìŒ")
                return {"error": "JSON íŒŒì‹± ì‹¤íŒ¨", "raw_response": response.text}
                
        except Exception as e:
            print(f"   âŒ LLM í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            return {"error": str(e)}
    
    def _create_dynamic_prompt(self, context: SearchContext) -> str:
        """ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ë™ì  í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        
        # ì´ë¯¸ì§€ ìœ ë¬´ì— ë”°ë¥¸ ì¡°ê±´ë¶€ í”„ë¡¬í”„íŠ¸
        image_instruction = ""
        if context.has_images:
            image_instruction = """
ì´ ë¬¸ì œëŠ” í‰ë¶€ X-ray ì´ë¯¸ì§€ì™€ í•¨ê»˜ ì œì‹œë  ì˜ˆì •ì…ë‹ˆë‹¤. 
ë¬¸ì œëŠ” ë°˜ë“œì‹œ ì´ë¯¸ì§€ í•´ì„ì´ í•„ìš”í•˜ë„ë¡ êµ¬ì„±í•˜ì„¸ìš”.
"""
        else:
            image_instruction = """
ì´ ë¬¸ì œëŠ” ì´ë¯¸ì§€ ì—†ì´ í…ìŠ¤íŠ¸ë§Œìœ¼ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤.
ì„ìƒ ì¦ë¡€ì™€ ê²€ì‚¬ ê²°ê³¼ ì„¤ëª…ì— ì§‘ì¤‘í•˜ì„¸ìš”.
"""
        
        # ì‹ ë¢°ë„ì— ë”°ë¥¸ ì¡°ê±´ë¶€ í”„ë¡¬í”„íŠ¸
        confidence_instruction = ""
        if context.confidence == "high":
            confidence_instruction = "ê²€ìƒ‰ëœ ì˜ë£Œ ì •ë³´ì˜ ì‹ ë¢°ë„ê°€ ë†’ìœ¼ë¯€ë¡œ, êµ¬ì²´ì ì´ê³  ì „ë¬¸ì ì¸ ë¬¸ì œë¥¼ ìƒì„±í•˜ì„¸ìš”."
        elif context.confidence == "medium":
            confidence_instruction = "ê²€ìƒ‰ëœ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•˜ë˜, ì¼ë°˜ì ì¸ ì˜í•™ ì§€ì‹ë„ í™œìš©í•˜ì—¬ ë¬¸ì œë¥¼ ë³´ì™„í•˜ì„¸ìš”."
        else:
            confidence_instruction = "ê²€ìƒ‰ ì •ë³´ê°€ ì œí•œì ì´ë¯€ë¡œ, í•´ë‹¹ ì£¼ì œì˜ ê¸°ë³¸ì ì´ê³  í•µì‹¬ì ì¸ ë‚´ìš©ìœ¼ë¡œ ë¬¸ì œë¥¼ êµ¬ì„±í•˜ì„¸ìš”."
        
        prompt = f"""ë‹¹ì‹ ì€ í•œêµ­ ì˜ì‚¬êµ­ê°€ê³ ì‹œ ë¬¸ì œ ì¶œì œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

# ì‚¬ìš©ì ìš”ì²­
ì‚¬ìš©ìê°€ "{context.query}"ì— ëŒ€í•œ ë¬¸ì œë¥¼ ìš”ì²­í–ˆìŠµë‹ˆë‹¤.

# ë²¡í„°DB ê²€ìƒ‰ ê²°ê³¼
ì¶”ì • ì˜ë£Œ ì£¼ì œ: {context.estimated_topic}
ê²€ìƒ‰ ì‹ ë¢°ë„: {context.confidence}

## ê´€ë ¨ ì˜í•™ ì§€ì‹
{context.text_content[:2000]}  

## ì´ë¯¸ì§€ ì •ë³´
{context.image_info}

# ìƒì„± ì§€ì¹¨

## ê¸°ë³¸ ìš”êµ¬ì‚¬í•­
1. í•œêµ­ ì˜ì‚¬êµ­ê°€ê³ ì‹œ í˜•ì‹ì˜ 5ì§€ì„ ë‹¤ ê°ê´€ì‹ ë¬¸ì œ 1ê°œ ìƒì„±
2. ì‹¤ì œ ì„ìƒ ìƒí™©ì„ ë°˜ì˜í•œ í™˜ì ì¦ë¡€ í¬í•¨
3. ìœ„ì˜ ê²€ìƒ‰ëœ ì˜ë£Œ ì§€ì‹ì„ ìµœëŒ€í•œ í™œìš©
4. í•œêµ­ ì˜ë£Œ í™˜ê²½ê³¼ ìš©ì–´ì— ë§ê²Œ ì‘ì„±

## ì¡°ê±´ë¶€ ì§€ì¹¨
{image_instruction}

{confidence_instruction}

## ì¶œë ¥ í˜•ì‹
ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”:

{{
    "question": "ë¬¸ì œ ë³¸ë¬¸ (í™˜ì ì¦ë¡€ í¬í•¨)",
    "options": ["ë³´ê¸°1", "ë³´ê¸°2", "ë³´ê¸°3", "ë³´ê¸°4", "ë³´ê¸°5"],
    "answer": ì •ë‹µ_ì¸ë±ìŠ¤_ìˆ«ì(0-4),
    "explanation": "ì •ë‹µ ê·¼ê±° ë° í•´ì„¤",
    "topic_analysis": {{
        "estimated_topic": "LLMì´ íŒë‹¨í•œ ì •í™•í•œ ì˜ë£Œ ì£¼ì œ",
        "difficulty_level": "ì´ˆê¸‰/ì¤‘ê¸‰/ê³ ê¸‰",
        "clinical_relevance": "high/medium/low",
        "requires_image": {str(context.has_images).lower()}
    }},
    "source_utilization": "ê²€ìƒ‰ëœ ì •ë³´ë¥¼ ì–´ë–»ê²Œ í™œìš©í–ˆëŠ”ì§€ ì„¤ëª…"
}}

ê²€ìƒ‰ëœ ì˜í•™ ì§€ì‹ì„ ì¶©ì‹¤íˆ ë°˜ì˜í•˜ì—¬ ì •í™•í•˜ê³  êµìœ¡ì  ê°€ì¹˜ê°€ ë†’ì€ ë¬¸ì œë¥¼ ìƒì„±í•˜ì„¸ìš”."""

        return prompt
    
    def generate_multiple_dynamic(self, queries: List[str], questions_per_query: int = 2) -> List[Dict]:
        """ì—¬ëŸ¬ ì¿¼ë¦¬ì— ëŒ€í•´ ë™ì  ë¬¸ì œ ìƒì„±"""
        
        print(f"\nğŸ”„ ë‹¤ì¤‘ ë™ì  ìƒì„±: {len(queries)}ê°œ ì¿¼ë¦¬")
        print("="*50)
        
        all_results = []
        
        for i, query in enumerate(queries):
            print(f"\nğŸ“‹ {i+1}/{len(queries)}: '{query}'")
            
            for j in range(questions_per_query):
                print(f"   {j+1}/{questions_per_query} ë²ˆì§¸ ë¬¸ì œ...")
                
                result = self.generate_question_from_query(query)
                
                if "error" not in result:
                    result["sequence"] = len(all_results) + 1
                    result["batch_query"] = query
                    all_results.append(result)
                    
                    topic = result["generated_question"].get("topic_analysis", {}).get("estimated_topic", "Unknown")
                    print(f"   âœ… ìƒì„± ì™„ë£Œ: {topic}")
                else:
                    print(f"   âŒ ìƒì„± ì‹¤íŒ¨: {result['error']}")
        
        print(f"\nğŸ¯ ì´ {len(all_results)}ê°œ ë¬¸ì œ ë™ì  ìƒì„± ì™„ë£Œ")
        return all_results
    
    def evaluate_dynamic_quality(self, results: List[Dict]) -> Dict[str, Any]:
        """ë™ì  ìƒì„± ë¬¸ì œë“¤ì˜ í’ˆì§ˆ í‰ê°€"""
        
        if not results:
            return {"error": "í‰ê°€í•  ê²°ê³¼ê°€ ì—†ìŒ"}
        
        print(f"\nğŸ“Š ë™ì  ìƒì„± í’ˆì§ˆ í‰ê°€ ({len(results)}ê°œ ë¬¸ì œ)")
        print("="*50)
        
        # í†µê³„ ìˆ˜ì§‘
        confidence_levels = []
        has_images_count = 0
        difficulty_distribution = {"ì´ˆê¸‰": 0, "ì¤‘ê¸‰": 0, "ê³ ê¸‰": 0}
        clinical_relevance = {"high": 0, "medium": 0, "low": 0}
        
        for result in results:
            # ê²€ìƒ‰ í’ˆì§ˆ
            search_context = result.get("search_context", {})
            confidence_levels.append(search_context.get("confidence", "low"))
            
            if search_context.get("has_images", False):
                has_images_count += 1
            
            # LLM ë¶„ì„ í’ˆì§ˆ
            question_data = result.get("generated_question", {})
            topic_analysis = question_data.get("topic_analysis", {})
            
            difficulty = topic_analysis.get("difficulty_level", "ì¤‘ê¸‰")
            if difficulty in difficulty_distribution:
                difficulty_distribution[difficulty] += 1
            
            relevance = topic_analysis.get("clinical_relevance", "medium")
            if relevance in clinical_relevance:
                clinical_relevance[relevance] += 1
        
        # í‰ê°€ ê²°ê³¼
        high_quality_count = len([c for c in confidence_levels if c == "high"])
        quality_rate = (high_quality_count / len(confidence_levels)) * 100
        
        evaluation = {
            "summary": {
                "total_questions": len(results),
                "high_confidence_rate": round(quality_rate, 1),
                "image_supported_rate": round((has_images_count / len(results)) * 100, 1),
                "average_confidence": self._calculate_avg_confidence(confidence_levels)
            },
            "quality_distribution": {
                "search_confidence": dict(zip(*zip(*[(c, confidence_levels.count(c)) for c in set(confidence_levels)]))),
                "difficulty_levels": difficulty_distribution,
                "clinical_relevance": clinical_relevance
            },
            "system_performance": {
                "vector_search_success": len([r for r in results if "error" not in r]),
                "llm_generation_success": len([r for r in results if "generated_question" in r]),
                "end_to_end_success_rate": round(len(results) / len(results) * 100, 1)
            }
        }
        
        # ê²°ê³¼ ì¶œë ¥
        print(f"ğŸ“ˆ ê³ ì‹ ë¢°ë„ ë¹„ìœ¨: {quality_rate:.1f}%")
        print(f"ğŸ–¼ï¸ ì´ë¯¸ì§€ ì§€ì›: {evaluation['summary']['image_supported_rate']:.1f}%")
        print(f"ğŸ¯ í‰ê·  ê²€ìƒ‰ í’ˆì§ˆ: {evaluation['summary']['average_confidence']}")
        
        return evaluation
    
    def _calculate_avg_confidence(self, confidence_levels: List[str]) -> str:
        """í‰ê·  ì‹ ë¢°ë„ ê³„ì‚°"""
        scores = {"high": 3, "medium": 2, "low": 1}
        avg_score = sum(scores.get(c, 1) for c in confidence_levels) / len(confidence_levels)
        
        if avg_score >= 2.5:
            return "high"
        elif avg_score >= 1.5:
            return "medium"
        else:
            return "low"
    
    def save_dynamic_results(self, results: List[Dict], filename: str = None) -> str:
        """ë™ì  ìƒì„± ê²°ê³¼ ì €ì¥"""
        
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"dynamic_generated_{timestamp}.json"
        
        save_dir = Path("generated_questions")
        save_dir.mkdir(exist_ok=True)
        
        filepath = save_dir / filename
        
        # ë©”íƒ€ë°ì´í„°ì™€ í•¨ê»˜ ì €ì¥
        save_data = {
            "metadata": {
                "generation_method": "dynamic_vector_search",
                "total_questions": len(results),
                "generation_date": datetime.now().isoformat(),
                "system_version": "Dynamic Generator v1.0",
                "vector_db": "Phase IV RAG System",
                "llm_model": "Gemini 1.5 Pro"
            },
            "results": results
        }
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(save_data, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ ë™ì  ìƒì„± ê²°ê³¼ ì €ì¥: {filepath}")
        return str(filepath)

# í¸ì˜ í•¨ìˆ˜ë“¤
def create_dynamic_generator() -> DynamicQuestionGenerator:
    """ë™ì  ìƒì„±ê¸° ì¸ìŠ¤í„´ìŠ¤ ìƒì„±"""
    return DynamicQuestionGenerator()

def quick_dynamic_generate(query: str = "íë ´") -> Dict:
    """ë¹ ë¥¸ ë™ì  ë¬¸ì œ ìƒì„±"""
    print(f"ğŸš€ ë¹ ë¥¸ ë™ì  ìƒì„±: '{query}'")
    
    try:
        generator = create_dynamic_generator()
        result = generator.generate_question_from_query(query)
        
        if "error" not in result:
            print(f"\nâœ… ë™ì  ìƒì„± ì„±ê³µ!")
            question_data = result["generated_question"]
            topic_analysis = question_data.get("topic_analysis", {})
            
            print(f"ğŸ“‹ ì¶”ì • ì£¼ì œ: {topic_analysis.get('estimated_topic', 'Unknown')}")
            print(f"ğŸ“Š ë‚œì´ë„: {topic_analysis.get('difficulty_level', 'Unknown')}")
            print(f"ğŸ¥ ì„ìƒ ê´€ë ¨ì„±: {topic_analysis.get('clinical_relevance', 'Unknown')}")
            print(f"ğŸ–¼ï¸ ì´ë¯¸ì§€ í•„ìš”: {topic_analysis.get('requires_image', False)}")
            
            return result
        else:
            print(f"âŒ ìƒì„± ì‹¤íŒ¨: {result['error']}")
            return result
            
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜: {e}")
        return {"error": str(e)}

def demo_dynamic_generation() -> None:
    """ë™ì  ìƒì„± ì‹œìŠ¤í…œ ë°ëª¨"""
    print("ğŸ¯ ë™ì  ë¬¸ì œ ìƒì„± ë°ëª¨")
    print("="*50)
    
    # ë‹¤ì–‘í•œ ì˜ë£Œ ì£¼ì œ í…ŒìŠ¤íŠ¸
    test_queries = [
        "íë ´ ì§„ë‹¨",           # ì´ë¯¸ì§€ + í…ìŠ¤íŠ¸
        "ê²°í•µ ì¹˜ë£Œ",           # í…ìŠ¤íŠ¸ ìœ„ì£¼
        "ê¸°í‰ ì‘ê¸‰ì²˜ì¹˜",       # ì´ë¯¸ì§€ + í…ìŠ¤íŠ¸
        "ì‹¬ë¶€ì „ ê´€ë¦¬",         # í…ìŠ¤íŠ¸ ìœ„ì£¼
        "íìƒ‰ì „ì¦ ì§„ë‹¨",       # í…ìŠ¤íŠ¸ ìœ„ì£¼
        "í‰ìˆ˜ ì²œììˆ "          # ì´ë¯¸ì§€ + í…ìŠ¤íŠ¸
    ]
    
    try:
        generator = create_dynamic_generator()
        results = generator.generate_multiple_dynamic(test_queries, questions_per_query=1)
        
        if results:
            filepath = generator.save_dynamic_results(results, "demo_dynamic.json")
            evaluation = generator.evaluate_dynamic_quality(results)
            
            print(f"\nğŸ‰ ë™ì  ìƒì„± ë°ëª¨ ì™„ë£Œ!")
            print(f"   ğŸ“ ì €ì¥: {filepath}")
            print(f"   ğŸ“Š ê³ ì‹ ë¢°ë„ ë¹„ìœ¨: {evaluation['summary']['high_confidence_rate']}%")
            print(f"   ğŸ–¼ï¸ ì´ë¯¸ì§€ ì§€ì›: {evaluation['summary']['image_supported_rate']}%")
            
        else:
            print("âŒ ë¬¸ì œ ìƒì„± ì‹¤íŒ¨")
            
    except Exception as e:
        print(f"âŒ ë°ëª¨ ì‹¤í–‰ ì‹¤íŒ¨: {e}")

if __name__ == "__main__":
    import sys
    from pathlib import Path
    
    if len(sys.argv) > 1:
        command = sys.argv[1].lower()
        
        if command == "test":
            query = sys.argv[2] if len(sys.argv) > 2 else "íë ´"
            quick_dynamic_generate(query)
            
        elif command == "demo":
            demo_dynamic_generation()
            
        else:
            print("ì‚¬ìš©ë²•:")
            print("  python dynamic_question_generator.py test [ì˜ë£Œì£¼ì œ]")
            print("  python dynamic_question_generator.py demo")
    
    else:
        print("ğŸ¤– ë™ì  ì˜ë£Œ ë¬¸ì œ ìƒì„± ì‹œìŠ¤í…œ")
        print("ë²¡í„°DB ê²€ìƒ‰ â†’ LLM ììœ¨ íŒë‹¨ â†’ ì ì‘í˜• ë¬¸ì œ ìƒì„±")